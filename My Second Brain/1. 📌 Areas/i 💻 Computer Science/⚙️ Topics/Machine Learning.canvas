{
  "nodes": [
    {
      "id": "b327174e89e37e5d",
      "type": "text",
      "text": "## **Clustering Report: Comparing K-Means and Hierarchical Clustering**\n\n### **1. Introduction**\nClustering is an **unsupervised learning** technique used to group similar data points together. In this analysis, we applied **K-Means** and **Hierarchical Clustering** on the **Mall Customers dataset**, using three features:\n- **Age**\n- **Annual Income (k$)**\n- **Spending Score (1-100)**\n\nThe goal was to segment customers based on their spending behavior and income levels.\n\n---\n\n### **2. Key Findings**\n| **Metric**       | **K-Means** | **Hierarchical Clustering** |\n|-----------------|------------|-----------------------------|\n| **Silhouette Score** | 0.4039 | 0.3614 |\n| **Number of Clusters Used** | 4 | 4 |\n| **Cluster Shape** | Spherical | Hierarchical/Tree-like |\n| **Performance** | Faster on large datasets | Slower due to pairwise distance calculations |\n| **Scalability** | Works well with large datasets | Computationally expensive for large datasets |\n| **Interpretability** | Easier | More interpretable with dendrograms |\n\n- **K-Means clustering** resulted in **slightly better-defined clusters** than Hierarchical Clustering, as indicated by the **higher silhouette score (0.4039 vs. 0.3614)**.\n- **Hierarchical clustering** provides a **dendrogram**, making it useful for visualizing relationships between customers.\n- **K-Means performed faster**, making it more suitable for large-scale clustering.\n\n---\n\n### **3. Visual Analysis**\n- The **scatter plots** for both methods revealed **distinct customer segments**.\n- The **Elbow Method** suggested that **4 clusters** was a reasonable choice.\n- The **Dendrogram** showed a hierarchical structure of clusters.\n\n---\n\n## **4. How to Improve Clustering Performance**\nSince the **Silhouette Scores** are relatively low, we can take the following actions to improve clustering results:\n\n### **4.1 Try Different Feature Scaling**\nK-Means and Hierarchical Clustering are affected by feature scales.  \nInstead of using raw values, apply **Standardization (Z-score normalization) or Min-Max Scaling** to make the feature contributions uniform.\n\n#### **Solution: Apply Standardization**\n```python\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardizing features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)  # Apply standardization to X\n```\n\n---\n\n### **4.2 Try Different Distance Metrics for Hierarchical Clustering**\nHierarchical clustering uses **Euclidean distance by default**, but other distance metrics like **Manhattan, Cosine, or Correlation** might perform better.\n\n#### **Solution: Use Different Linkage Methods**\n```python\n# Try different linkage methods\nhc_complete = AgglomerativeClustering(n_clusters=4, affinity=\"euclidean\", linkage=\"complete\")\nhc_average = AgglomerativeClustering(n_clusters=4, affinity=\"euclidean\", linkage=\"average\")\n```\n\n---\n\n### **4.3 Optimize K-Means Initialization**\nK-Means is sensitive to **cluster initialization**. The **k-means++** initialization helps but sometimes trying **different values of `n_init`** (number of centroid initializations) can improve results.\n\n#### **Solution: Increase `n_init`**\n```python\n# Use more centroid initializations\nkmeans = KMeans(n_clusters=4, init='k-means++', n_init=20, random_state=42)\nkmeans.fit(X_scaled)  # Using the standardized features\n```\n\n---\n\n### **4.4 Use Dimensionality Reduction (PCA)**\nIf the dataset has redundant features, **Principal Component Analysis (PCA)** can help reduce noise and improve cluster separability.\n\n#### **Solution: Apply PCA Before Clustering**\n```python\nfrom sklearn.decomposition import PCA\n\n# Reduce dimensions before clustering\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# Apply K-Means on PCA-transformed data\nkmeans_pca = KMeans(n_clusters=4, random_state=42)\nkmeans_pca.fit(X_pca)\n```\n\n---\n\n### **4.5 Use a Different Clustering Algorithm**\nIf K-Means and Hierarchical Clustering do not perform well, other algorithms like **DBSCAN** (Density-Based Clustering) or **Gaussian Mixture Models (GMM)** might work better.\n\n#### **Solution: Try DBSCAN**\n```python\nfrom sklearn.cluster import DBSCAN\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=0.5, min_samples=5)\ndbscan_labels = dbscan.fit_predict(X_scaled)\n```\n\n---\n\n## **5. Conclusion**\n- **K-Means performed better** than Hierarchical Clustering in terms of Silhouette Score.\n- Applying **standardization, optimizing hyperparameters, and trying alternative clustering methods** can improve performance.\n- **Dimensionality reduction (PCA)** can help remove noise and improve clustering quality.\n\nðŸ“Œ **Next Steps:** Implement these changes and evaluate their impact on the **Silhouette Score**! ðŸš€",
      "styleAttributes": {},
      "x": -440,
      "y": -300,
      "width": 660,
      "height": 520
    }
  ],
  "edges": [],
  "metadata": {}
}